{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e5786dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk, re, os, glob\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import collections\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "spell = SpellChecker()\n",
    "# nltk.download('stopwords')      #First time use\n",
    "# stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec00adfa",
   "metadata": {},
   "source": [
    "# Set Input Original Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b411af53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\real_research\\step2\\data1\n"
     ]
    }
   ],
   "source": [
    "input_path = r\"F:\\real_research\\step2\\data1\"\n",
    "print(input_path)\n",
    "all_files = glob.glob(os.path.join(input_path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a77b",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4faea650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e963f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "def clean_data(file):\n",
    "    data = pd.read_csv(file,encoding= 'unicode_escape',error_bad_lines=False)\n",
    "    data.columns = [\"\",\"\",\"\",\"text\",\"favorited\",\"favoriteCount\",\"replyToSN\",\"created\",\"truncated\",\"replyToSID\",\"id\",\"replyToUID\",\"statusSource\",\"screenName\",\"retweetCount\",\"isRetweet\",\"retweeted\",\"longitude\",\"latitude\"]\n",
    "    data = data['text']\n",
    "#     i = 0\n",
    "    spellcheck = []\n",
    "    for tweet in data:\n",
    "#         i = i + 1\n",
    "        tokens = word_tokenize(tweet)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        str_tmp = \"\"\n",
    "        st = []\n",
    "#         header = [i]\n",
    "        for wordwithtag in tagged:   \n",
    "            if (\"NN\" == wordwithtag[1]) or (\"NNP\" == wordwithtag[1]) or (\"NNS\" == wordwithtag[1]) or (\"NNPS\" == wordwithtag[1]):\n",
    "                tmp = wordwithtag[0]\n",
    "                tmp = re.sub(r'[^\\w\\s]', '', tmp)\n",
    "#                 print(i, \"the tag is :    \",wordwithtag[1],\"  and the word is:    \",tmp)\n",
    "                st.append(tmp)\n",
    "            spellchecked = spell.known(st) \n",
    "#             spellchecked = header + list(spellchecked)  \n",
    "            str_tmp = ','.join([str(elem) for elem in spellchecked])               \n",
    "        \n",
    "        spellcheck.append(spellchecked)\n",
    "\n",
    "        \n",
    "        \n",
    "    tmpfile = file\n",
    "#     print(tmpfile)\n",
    "    string2 = 'F:\\\\real_research\\step2\\\\data1\\\\'\n",
    "    string3 = '.csv'\n",
    "#     print(string2)\n",
    "#     filename = (tmpfile.replace(string2,'')).replace(string3,'.xlsx')\n",
    "    filename = (tmpfile.replace(string2,'')).replace(string3,'')\n",
    "    tmp = filename\n",
    "#     for letter in filename:\n",
    "#         print(letter)\n",
    "#         print(\"tmp is\",tmp)\n",
    "#         if letter.isalpha():\n",
    "#     tmp = tmp.replace(letter,'')\n",
    "#             print(\"after\",tmp)\n",
    "    filename = tmp + '.xlsx'\n",
    "       \n",
    "    filename = \"clean_data\\\\\"+filename\n",
    "#     print(filename)\n",
    "    pd.DataFrame(spellcheck).to_excel(filename, header=False, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "for file in pbar(all_files):  \n",
    "    clean_data(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# for file in all_files:\n",
    "#     clean_data(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab82d413",
   "metadata": {},
   "source": [
    "# Set Input Clean Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b13691a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\real_research\\step2\\clean_data\n"
     ]
    }
   ],
   "source": [
    "input_path = r\"F:\\real_research\\step2\\clean_data\"\n",
    "print(input_path)\n",
    "all_files_xlsx = glob.glob(os.path.join(input_path, \"*.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec16b07",
   "metadata": {},
   "source": [
    "# Find the most 25 frequnt Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "419cfa5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "listmost = []\n",
    "pbar1 = ProgressBar()\n",
    "def frequentwords(file):\n",
    "    all_clean_data = []\n",
    "    listmost1 = []\n",
    "    pd.read_excel(file,index_col = 0)\n",
    "    clean_data = pd.read_excel(file,index_col=None, header=None, keep_default_na = False) \n",
    "#     print(clean_data)\n",
    "    list1 = clean_data.values.tolist()\n",
    "    \n",
    "    all_clean_data = []\n",
    "    for tweet in list1:\n",
    "        for words in tweet:\n",
    "            if words != '':\n",
    "                all_clean_data.append(words)    \n",
    "#     print(all_clean_data)    \n",
    "    count = Counter(all_clean_data)\n",
    "#     print(count.most_common(24))\n",
    "    for word_count in count.most_common(24):\n",
    "        for word_or_count in word_count:\n",
    "            listmost1.append(word_or_count)\n",
    "    \n",
    "#     listmost1 = count.most_common(24)\n",
    "\n",
    "    tmpfile = file\n",
    "    string2 = 'F:\\\\real_research\\step2\\\\clean_data\\\\'\n",
    "    string3 = '.xlsx'\n",
    "#     print(string2)\n",
    "    date = (tmpfile.replace(string2,'')).replace(string3,'')\n",
    "    date = [date]\n",
    "    \n",
    "    \n",
    "    listmost1 = date + listmost1\n",
    "    \n",
    "    \n",
    "    \n",
    "    listmost.append(listmost1)\n",
    "#     print(listmost)\n",
    "\n",
    "\n",
    "    file_path = 'F:\\\\real_research\\\\step2\\\\frequent\\\\frequent_words.xlsx'\n",
    "    \n",
    "    df = pd.DataFrame(listmost)\n",
    "    \n",
    "    \n",
    "    start_row = df.shape[0]\n",
    "#     df = listmost\n",
    "    writer = pd.ExcelWriter(file_path, engine='xlsxwriter')\n",
    "    df.to_excel(writer, startrow=0, merge_cells=False, sheet_name=\"Summary\", index=False)\n",
    "    \n",
    "    writer.save()\n",
    "    \n",
    "#     for df in listmost1:  # assuming they're already DataFrames\n",
    "#         df.to_excel(startrow=start_row, index=False)\n",
    "#         start_row += len(df) + 1  # add a row for the column header?\n",
    "#     writer.save()  # we only need to save to disk at the very end!\n",
    "    \n",
    "    \n",
    "#     file_shape = df.shape[0] # <<-- This saves the count of rows to a variable\n",
    "#     df['file_shape'] = file_shape\n",
    "#     writer = pd.ExcelWriter(output, engine='xlsxwriter')\n",
    "#     df.to_excel(writer, startrow=0, merge_cells=False, sheet_name=\"Summary\", index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "#         writer.book = load_workbook(file_path)\n",
    "#         data_filtered.to_excel(writer, \"Main\", cols=[1,1])\n",
    "    \n",
    "\n",
    "\n",
    "    pd.DataFrame(listmost).to_excel(file_path, header=False, index=False)\n",
    "    \n",
    "    \n",
    "# i = 0\n",
    "for file_xlsx in pbar1(all_files_xlsx):\n",
    "#     i = i+1\n",
    "#     print(i,file_xlsx)\n",
    "    frequentwords(file_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb65b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
